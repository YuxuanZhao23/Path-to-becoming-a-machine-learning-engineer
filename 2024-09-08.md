# Random Tree

- bootstrapped dataset 有放回的取样
- 每一步，只使用 variable 的 subset，使用其中最能够用于区分的那一个variable。然后在下一个时刻，我们在除了这个 variable 以外的 variable 里选取 subset：这个subset size是一个可以调节的hyper parameter
- 我们可以把没有被选中的样本当作测试，得到 out of bag error

# Cross Validate

我们想要 maximize dataset 可用于 training 和 testing 的数量

k fold 就是把数据 k 等分，然后运行k次训练，每次使用一个不同的 $\frac{1}{k}$ 的数据作为 testing set。然后我们需要 average 这 k 次训练的结果

# Epoch & batch

Epoch 是训练过程中看了多少遍数据
batch 是一部分的数据，训练时看过整个 batch 之后根据 error 就会更新 weights

较小的 batch size 允许模型更快地学习，但由于梯度方差（gradient variance）较大，可能会引入更多噪音，而较大的批次可以提供更高的学习稳定性，但代价是学得慢