### 2.1.1

2.1 里面定义的 log likelihood 

$$l(\theta) = log L(\theta) = \sum\limits_{i=1}^{n}y^{(i)} log (h(x^{(i)})) + (1 - y^{(i)})log(1 - h(x^{(i)})$$

定义 $t = \theta^Tx$ 代入 $h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$ 可以得到 $h_\theta(x) = \frac{1}{1 + e^{-t}}$，将其代入到 log likelihood 得到（第三行：因为 $log(a^b) = b * log(a)$ ，所以有 $log \frac{1}{x} = - log (x)$）

$$
\begin{aligned}
l(\theta) &= \sum\limits_{i=1}^{n}y^{(i)} log \frac{1}{1 + e^{-t}} + (1 - y^{(i)})log \frac{1 + e^{-t} - 1}{1 + e^{-t}}\\ &= \sum\limits_{i=1}^{n}y^{(i)} log \frac{1}{1 + e^{-t}} + (1 - y^{(i)})log \frac{1}{1 + e^{t}}\\ &= -\sum\limits_{i=1}^{n}y^{(i)} log (1 + e^{-t}) - (1 - y^{(i)}) log (1 + e^{t})
\end{aligned}$$

求导 

这里面的 $l_{logistic}(t, y)$ 其实是二元的 entropy function，所以下面多元的变成 cross entropy

$$
\begin{aligned}
\frac{\partial l(\theta)}{\partial \theta} &= -\frac{\partial l_{logistic}(t, y)}{\partial t} \frac{\partial t}{\partial \theta}\\ &= (- y \frac{-e^{-t}}{1 + e^{-t}} - (1-y)\frac{e^t}{1 + e^{t}})x\\ &= (- y \frac{-e^{-t}}{1 + e^{-t}} - (1-y)\frac{1}{1 + e^{-t}})x\\ &= (\frac{y e ^{-t} - 1 + y}{1 + e^{-t}})x\\ &= (y - \frac{1}{1 + e^{-t}})x\\ &= (y - h_\theta(x))x
\end{aligned}$$

# 3 Generalizaed Linear Model

linear regression 有着 $y|x; \theta \sim N(\mu, \sigma^2)$ 的正态分布 (normal distribution)

linear classification 有着 $y|x; \theta \sim Bernoulli(\phi)$ 的伯努利分布

y 服从正态分布/伯努利分布，$\phi, \mu, \sigma$ 是和 $(x, \theta)$ 有关系的

## 3.1 Exponetial Family

定义 exponential family: $p(y;\eta) = b(y)e^{\eta^TT(y) - a(\eta)}$

- $\eta$: natural parameter
- $T(y)$: sufficient statistic 通常是 $T(y) = y$
- $a(\eta)$: log partition function，这一项主要是 normalization constant 的作用，使得分布的和是等于一
- 确定的 $T, a, b$ 定义了一个 distribution family，不同的 $\eta$ 是这个 family 里的不同分布

### 伯努利分布

$$
\begin{aligned}
p(y; \phi) &= \phi^y(1-\phi)^{1-y}\\
&= e^{log(\phi^y(1-\phi)^{1-y})}\\
&= e^{log\phi^y + log(1-\phi)^{1-y}}\\
&= e^{ylog\phi + (1-y)log(1-\phi)}\\
&= e^{ylog\phi -ylog(1-\phi) + log(1-\phi)}\\
&= e^{y(log\phi -log(1-\phi)) + log(1-\phi)}\\
&= e^{y(log\frac{\phi}{1-\phi}) + log(1-\phi)}\\
\end{aligned}
$$

$$\eta = log\frac{\phi}{1 - \phi} \rightarrow \phi = \frac{1}{1 + e^{-\eta}}$$

套公式就可以得到

$$b(y) = 1, T(y) = y$$

$$\begin{aligned}a(\eta) &= -log(1-\phi)\\
&= -log(1-\frac{1}{1 + e^{-\eta}})\\
&= -log(\frac{1 + e^{-\eta} - 1}{1 + e^{-\eta}})\\ 
&= -log(\frac{e^{-\eta}}{1 + e^{-\eta}})\\
&= log(\frac{1 + e^{-\eta}}{e^{-\eta}})\\
&= log(\frac{1}{e^{-\eta}} + 1)\\
&= log(e^\eta + 1)\\
\end{aligned}$$

### 高斯分布

$$\begin{aligned}
p(y;\mu) &= \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(y-\mu)^2}\\
&= \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(y^2 + \mu^2 - 2y\mu)}\\
&= \frac{1}{\sqrt{2\pi}}e^{(-\frac{y^2}{2} + y\mu -\frac{\mu^2}{2})}\\
&= \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}e^{y\mu -\frac{\mu^2}{2}}\\
\end{aligned}$$

$$\eta = \mu, T(y) = y$$
$$a(\eta) = \frac{\eta^2}{2}$$
$$b(y) = \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}$$

## 3.2 构建 GLM

1. $y|x; \theta \sim ExponetialFamily(\eta)$: 给定 $(x, \theta)$，那么 $y$ 的分布需要服从某种 $\eta$ exponential family distribution 
2. 因为绝大多数时候 $T(y) = y$，所以我们需要有 $h(x) = E[y|x]$
3. $\eta = \theta^Tx$: $\eta$ 和 $x$ 是线性关系的，这是一种 design choice

### 3.2.1 Ordinary Least Squares

1. y 服从高斯分布 $N(\mu, \sigma^2)$
2. $h_\theta(x) = E[y|x; \theta] = \mu = \eta = \theta^Tx$

### 3.2.2 Logistic Regression

1. logistic regression 服从伯努利分布
2. 
$$\begin{aligned}
h_{\theta}(x) &= E[y|x; \theta]\\
&= \phi\\
&= \frac{1}{1 + e^{-\eta}}\\
&= \frac{1}{1 + e^{-\theta^Tx}}
\end{aligned}$$

- canonical response function: 返回 distribution mean $g(\eta) = E[T(y); \eta]$
- canonical link function: $g^{-1}$

# LLM

- pretraining: GPT3/ GPT2
- post-training: ChatGPT

## LM

token sequence's probability distribution，如果有 syntactic 或者 semantic error 的话 probability 应该很低

Autoregressive LM: $p(x_1, ..., x_L) = p(x_1)p(x_2|x_1)p(x_3|x_2, x_1)... = \displaystyle\prod_{i}p(x_i|x_1: x_{i-1})$

## steps

1. tokenize: 一般会变成3-4个字符的长度，同一个token可以有多个含义
2. forward
3. next token probability distribution (training 到这一步)
4. sample
5. detokenize

## loss

cross entropy loss，损失最小化等同于下一个token likelihood最大化

$$max \displaystyle\prod_{i}p(x_i|x_1:x_{i-1}) = min(-\displaystyle\sum_i log(p(x_i|x_i: x_{i-1})) = min(L(s_i: x_L))$$

## Evaluation

perplaxity 困惑度 $PPL(x_1: x_L) = 2 ^ {\frac{L(x_1: x_L)}{L}} = \prod p(x_i|x_1: x_{i-1})^{-\frac{1}{L}}$
- 每个 token 的平均值，独立于 sequence 长度
- exponentiate：独立于 log base
- range 1 ~ |Vocab|：直觉就是 model 在多少个 token 之间犹豫，1的时候就是模型非常确定是某一个
- 大概从 ~70 改进到 ~10 了，但是现在很少被使用因为 tokenize 的方法不同会无法比较
- 现代方法：所有常见任务的集合 MMLU (HEML, Harness, Original)
- 学术上需要担心 train & test contamination 污染，工业界不管

## Data

1. 网络爬虫下载整个互联网：Common Crawl 250 billion pages, > 1PB
2. extract from html: math, boiler plate
3. undesirable: NSFW, PII, harmful
4. duplicate
5. Heuristic filter: 保留高质量内容
6. 制作一个简单 ML 来分辨是否一个页面可以被 wikipedia 引用（质量足够好）
7. data mix：对内容 category 的比重进行调节（比如说不想要有太多 entertainment 的内容，想要更多 code/ book）
8. 最后阶段 overfit 很好的内容：比如说 wikipedia
9. 处理数据的效率，multi-modal data 多模态数据，synthetic data 合成数据？
10. copyright liability: book
11. 常见的数据库：fineWeb 15T tokens

## System

- GPU 的 throughput 是我们优化的重点
- 矩阵乘法是其他的 fp ops 速度的十倍
- 如果不优化代码，GPU绝大多数时候都是 idle，Model Flop Utilization 能达到接近理论最优的性能的 50% 就已经很好了
- 低精度：矩阵乘法使用 bf16 而不是 fp32，一般使用 automatic mixed precision：存储的权重是 fp32，但是计算前转化成 bf16
- opertion fusion：数据在 DRAM 和 SRAM 之间反复来回，所以使用 ```torch.compile``` 来优化，速度可以快两倍
- tiling：使用 subphase 在矩阵乘法中来复用cache里的数字，增加cache hit
- 例子：kernel fusion + tiling + recomputation for attention: FlashAttention 1.7x speed
- split memory and compute across GPUs: 一个 P parameters 的模型需要 16P 的 DRAM：4P model + 2 * 4P optimizer + 4P gradient
- sharding：每一个 GPU 都有 weight subset
- pipeline parallel: 每一个 GPU 是一个单独的 layer
- tensor parallel: 一个矩阵分散到多个 GPU 上，使用partial sum
- mixture of experts：使用 selector alyer 来使用更少的 active parameters：原理是不是每一个datapoint都需要进入每一个parameter

## Scaling Laws

更大的模型，更多的数据不会导致 overfitting，那么我们能预测某个数据量和模型大小下的性能吗？

正因为有 scaling laws 所以我们不会尝试在所有数据上使用不同的 hyperparameters 组合，而是先找到scaling recipes（比如说学习率应该随着size增大而减少）。然后在较小的模型上尝试不同的hyperparameters和大小的组合，然后直接使用相同的hyperparameters组合到更大的模型上（训练更长的时间）

因为 scaling law 存在，所以提升 intersect 是不如提升斜率来得有效的。所以改变 loss 的定义远远不如增加更多的数据

那么我们该用多少数据和parameter呢？在FLOPs上parameter和token的log表现也是线性的，两者的比例大概是20:1 token: parameter，现实中一般选用 >150:1

不要花时间 over complicating，做简单的事情然后 scale

## Training LLAMA 3

15.6T tokens: 405B parameters => 40: 1

FLOPs: 6 * 15.6T * 405B = $3.8 \times 10 ^ {25}$ FLOPs 法律规定 $10 ^ {26}$ 需要接受严格审查

时间：70天
花费：65 - 85M

## Post Training

Supervised Finetuning (SFT)

pretrain LLM 给出的句子是不能回答用户的问题的，我们需要finetune LLM来给出我们想要的答案

最开始使用人给出的数据，但是这样太贵太慢了，所以后来让LLM自己生成类似的数据

不需要太多 SFT 数据，只需要几千就好了，因为其实LLM已经学会了，只是告诉它需要给出哪种类型的答案

## Reinforcement Learning from Human Feedback RLHF

1. 人类很容易 distinguish 但是不能 generate
2.  Hallucination 幻觉
3.  price
4.  给两个不同的结果，让人类去选哪一个他们更喜欢
5.  reward model R 使用一个 logistic regression 来找到用户的偏好，因为这个R是连续的而不是binary的所以我们有更多的数据
6.  $p(i > j) = \frac{e^{R(x, \hat{y}_i)}}{e^{R(x, \hat{y}_i)} + e^{R(x, \hat{y}_j)}}$
7.  使用 PPO 来优化 $E_{\hat{y} \sim p_\theta(\hat{y}|x)}[R(x, \hat{y}) - \beta log\frac{p_\theta(\hat{y}|x)}{p_{ref}(\hat{y}|x)}]$ 后面这一项是用来 regularization 避免过拟合的
8.  开源中最新的做法是 DPO 最大化 perferred output 的可能性，最小化其他的，也就是说我们简化了 RL 的 reward model 和各种复杂的操作 $L_{DPO}(\pi_\theta; \pi_{ref}) = -E_{(x, y_w, y_l) \sim D}[log (\sigma(\beta log\frac{\pi_\theta(y_w |x)}{\pi_{ref}(y_w | x)} - \beta log\frac{\pi_\theta(y_l | x)}{\pi_{ref}(y_l | x)}))]$
9.  使用RHLF之后的一个显著特征是答案会变长，两个类似的答案，人类往往会选择更长的那一个。
10. RHLF的数据收集也可以使用 LLM 来生成而不是真的人类，因为LLM有更小的variance和潜在更大的bias，实际表现是出人意料地很好

## Evaluation

1. 不能用 validation loss
2. 不能用 perplexity
3. 答案的 diversity 很大
4. 任务是 open-ended，很难 automate
5. 我们同样可以让 annotator 来选择 preference: chatbot arena
6. 同样可以使用 LLM 来处理
7. 大概有 1M 的数据，比SFT要多很多


# 4 Generative Learning Algorithms

generative learning model: 不是找到一个线性的 boundary，而是对每一类单独找出其特征范围，然后将新的输入和每一个范围进行对比找出最像的一类 (e.g. $p(x|y=0), p(x|y=1)$)

discriminative learning algorithm: 尝试学习 $p(y|x)$ 和找到 $X \rightarrow \{0, 1\}$ 映射

我们的目标是找到 $p(y), p(x|y)$ 然后使用 Bayes rule 计算得出: $p(y|x) = \frac{p(x|y)p(y)}{p(x)}$

一个二分类的例子是 $p(x) = p(x|y=1)p(y=1) + p(x|y=0)p(y=0)$

argmax 的定义是将最大的一项设为1，所有其他的项设为0

因为 $\underset{y}{\mathrm{max}}\, f(x)g(y) = f(x)\underset{y}{\mathrm{max}}\, g(y)$

所以 $\underset{y}{\mathrm{argmax}}\, f(x)g(y) = \underset{y}{\mathrm{argmax}}\, g(y)$

同样的：
$$
\begin{aligned}
\underset{y}{\mathrm{argmax}}\, p(y|x) &= \underset{y}{\mathrm{argmax}}\, \frac{p(x|y)p(y)}{p(x)}\\
&= \underset{y}{\mathrm{argmax}}\, p(x|y)p(y)
\end{aligned}
$$

## 4.1 Gaussian Discriminant Analysis 正态分布

假定 $x \in R^n$（这里我们不要 $x_0 = 1$ 的惯例了）和 $p(x|y)$ 是高斯分布的

多元高斯分布：$z \sim N(\vec{\mu}, \Sigma)$，同时 $\vec{\mu} \in R^n, \Sigma \in R^{n \times n}, z \in R^n$

那么分布的期望值是 $E[z] = \mu$

分布的协方差 covariance:

$$
\begin{aligned}
Cov(z) &= E[(z - \mu)(z - \mu)^T]\\
&= E[(z - \mu)(z^T - \mu^T)]\\
&= E[zz^T - z\mu^T - \mu z^T + \mu \mu ^T]\\
&= E[zz^T] - E[z]\mu^T - \mu E[z^T] + \mu \mu^T\\
&= E[zz^T] - \mu\mu^T - \mu E[z^T] + \mu \mu^T\\
&= E[zz^T] - \mu E[z^T]\\
&= E[zz^T] - E[z]E[z]^T
\end{aligned}
$$

probability density function for Guassian:

不需要死记硬背: $p(x; \mu, \Sigma) = \frac{e^{-\frac{(x - \mu)^T\Sigma^{-1}(x - \mu)}{2}}}{(2\pi)^{\frac{d}{2}}|\Sigma|^{\frac{1}{2}}}$

一个标准的 $\Sigma = \left[\begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix}\right]$

如果我们改成 $\left[\begin{matrix} 2 & 0 \\ 0 & 2 \end{matrix}\right]$，将会更平但覆盖更多的面积

如果改成$\left[\begin{matrix} 0.5 & 0 \\ 0 & 0.5 \end{matrix}\right]$则会更窄更尖

如果改成$\left[\begin{matrix} 1 & 0.8 \\ 0.8 & 1 \end{matrix}\right]$ 会变得又尖又扁，也就是说 $z_1, z_2$ 更加的正相关（知道 $x_1$ 就几乎知道 $x_2$）

如果改成$\left[\begin{matrix} 1 & -0.8 \\ -0.8 & 1 \end{matrix}\right]$ 形状不变，收窄的方向旋转90度，现在的 $z_1, z_2$ 是负相关

$\Sigma$ 是对称的 ($\Sigma^T = \Sigma$) 和 semi-definite ($\forall x \in R^d, x^T\Sigma x \geq 0$)

### 4.1.2

- 第二三行是假设的还是推出来的？
- 为什么 $\Sigma$ 对于不同的类的分布是一样的？（分布的形状为什么一样呢？）因为这样得到的boundary就不是linear的，而且我们需要的参数翻倍

## 4.2 Naive Bayes

x 输入是离散的，之前都是连续的

$$
\begin{aligned}
p(x, ..., x_{10000}|y) &= p(x_1|y)p(x_2|x_1, y)p(x_3|x_1,x_2,y)...p(x_{10000}|...)\\
\text{conditional independent assume} &= p(x_1|y)p(x_2|y)p(x_3|y)...p(x_{10000}|y)\\
&= \displaystyle\prod_{i=1}^n p(x_i|y)
\end{aligned}
$$

数据往往不是高斯分布的（？），所以这个假设是错误的，但是往往错得不离谱

joint likelihood: $L(\phi_y, \phi_{j|y}) = \displaystyle\prod_{i=1}^m p(x^{(i)}, y^{(i)}; \phi_y, \phi_{j|y})$

MLE 之后:

$$
\begin{aligned}
p(y=1) = \phi_y &= \frac{\displaystyle\sum_{i=1}^m 1 \{y^{(i)} = 1\}}{m}\\
p(x_j=1|y=1) = \phi_{j|y=1} &= \frac{\displaystyle\sum_{i=1}^m 1 \{x_j^{(i)} = 1, y^{(i)} = 1\}}{\displaystyle\sum_{i=1}^m 1\{y^{(i)} = 1\}}
\end{aligned}
$$

不需要fit，也很容易用新数据来update，公式也符合直觉

predict:

$$
\begin{aligned}
p(y=1|x) &= \frac{p(x|y=1)p(y=1)}{p(x)}\\
&= \frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0)p(y=0)}
\end{aligned}
$$

### 4.2.1 Laplace smoothing

如果分子/分母是0的话（没见过），概率就为0。但是只因为自己没有见过就说这件事不可能发生是非常片面的

$$\phi_j = \frac{1 + \displaystyle\sum_{i=1}^n 1 \{z^{(i)} = j\}}{k + n}$$

k 是有多少个 class，这里是 binary

$$\phi_{j|y=1} = \frac{1 + \displaystyle\sum_{i=1}^n 1 \{x_j^{(i)} = 1 | y^{(i)} = 1\}}{2 + \displaystyle\sum_{i=1}^n 1 \{y^{(i)} = 1\}}$$

### 4.2.2 Multivariate Bernoulli Event Model vs Multinomial Even Model

有一个从单词到id的映射，现在每个邮件都是单词长度的vector，按单词顺序的写它们 的id

$d = |email|, k = |vocabulary|$

$p(y=1, x) = p(y)\displaystyle\prod_{j=1}^d p(x_j|y)$

假定某个词出现的概率独立于这个词出现的位置

$\phi_{k|y=0} = p(x_j = k | y = 0)$

MLE:

$\phi_{k|y=0} = \frac{\displaystyle\sum_{i=1}^m I \{y^{(i)} = 0\}\displaystyle\sum_{j=1}^{d_i} I \{x_j^{(i)} = k\}}{\displaystyle\sum_{i=1}^m I \{y^{(i)} = 0\} d_i}$

- 分母是所有非spam email的长度之和
- 分子是所有的非spam email里word k出现了多少次
- 同样可以在这里使用 laplace smoothing，分母加上的同样是k，但这里代表词汇表有多少个单词

# 5 Kernel Method

## 5.1 Feature Map

attribute 是原始输入

feature 是映射的新变量

feature map $\phi$ 是从 attribute 到 feature

## 5.2 LMS

类似于之前的线性模型的sgd（链式法则），我们有

$$\theta := \theta + \alpha (y^{(i)} - \theta ^ T \phi (x^{(i)})) \phi (x^{(i)})$$

## 5.3 LMS with the kernel trick

why optimal weight vector w can be expressed as a linear combination of the support vectors? 文中的 $\beta$ 往往被称作 lagrange multiplier

计算 $\phi$ 消耗太大，所以我们想办法把上面优化 $\phi$ 成计算 $\phi(x^{(i)})^T\phi(x^{(j)})$，而这个东西就是 kernel，它能够降维

$$
\begin{aligned}
\phi(x) &= x \Rightarrow \\ k(x^{(i)}, x^{(j)}) &= <x^{(i)}, x^{(j)}>
\end{aligned}$$

$$
\begin{aligned}\phi(x) &= (1, x_1, x_2, x_1x_2, x_2x_1, x_1^2, x_2^2) \Rightarrow \\ k(x^{(i)}, x^{(j)}) &= 1 + <x^{(i)}, x^{(j)}> + <x^{(i)}, x^{(j)}>^2
\end{aligned}$$

$$
\begin{aligned}
\phi(x) &= (1, x_1, x_2, x_3, x_1x_2, x_1^3, x_1^2x_2, ...) \Rightarrow \\ k(x^{(i)}, x^{(j)}) &= 1 + <x^{(i)}, x^{(j)}> + <x^{(i)}, x^{(j)}>^2 + <x^{(i)}, x^{(j)}>^3
\end{aligned}$$

什么是inner product？就是两个相同长度的向量做 element-wise multiply之后求和：类似于 Hadamard product，但是 Hadamard 不求和

既然计算只用 kernel 就行，那我们还需要显式写出 $\phi$ 吗？只需要确保 $\phi$ 的存在就好

# 6 SVM

## optimal margin classifier

functional margin：
- if y = 1, hope that $\theta^Tx >> 0$
- if y = 0, hope that $\theta^Tx << 0$

## Duality

$$
\theta(w) = max_\beta L(w, \beta) = f(w) + \displaystyle\sum_{i=1}^l \beta_i h_i(w)
\left \{
\begin{array}{c}
f(w), h(w) = 0\\
+\infty, h(w) \neq 0
\end{array}
\right.
$$

$$
f(w) = min_w \theta(w)
$$