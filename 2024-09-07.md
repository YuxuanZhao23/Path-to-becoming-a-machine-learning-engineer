# Deep Network Tuning

DL 是一种从数据中挖掘信息的编程语言，一开始定下的模版，里面所有的value不需要一开始定，同时是differentiable

## Batch Normalization（统计上其实是 standardization）

归一化 standardize: average = 0, variance = 1

让损失函数更加的平滑：$|| \bigtriangledown f(x) - \bigtriangledown f(y) || ^ 2 \leq \beta || x - y || ^ 2$

平滑有什么用？好处就是学习率可以大一点，如果很不平滑的话，每走一步的终点 y 就已经和起点 x 的斜率有很大差别，也就不该用同一个斜率走这么远的距离。如果可以用一个上限 bound 住的话，就证明梯度改变得不大

$\beta$ 越小，学习率就可以取得越大。batch normalization 就是想要平滑 internal layer（归一化原本只作用于第一个线性层）不会改变结果，只影响学习率

1. Reshape 变成 2D（比如说 CNN 一般就是四维的，batch size, RGB channel, height, width 压缩成 channel 和其他）
2. Normalize：$\hat{x}_j' \leftarrow (x_j' - mean(x_j'))/std(x_j')$
3. Recovery：如果我们想要保留方差的话，可以通过 $y_j' = \gamma_j\hat{x}_j + \beta_j$ 还原回去，$\gamma_j, \beta_j$ 是可以学的参数
4. 把 reshape 后的 $Y'$ output

实现上其实不会使用真正的均值，而是均值平滑，因为这样会比较容易写

```python
moving_mean = momentum * moving_mean + (1 - momentum) * mean
moving_var = momentum * moving_var + (1 - momentum) * var
```

缺点：在 GAN 和 adversarial attack 里不好用（因为里面的均值和方差是不稳定的）

使用 BN 不会影响精度，主要是将训练过程变得平滑一点。在 CNN 里面不用 BN 也是没关系的

## Layer Normalization

对于 RNN 来说，均质和方差应当在每一个 time step 是独立的，如果 inference 的时候 sequence 长度比训练的时候长那就会有问题

二维的时候进行转置，四维的时候还是把n放在后面，cwh放在前面

所以之前我们是对这一列的特征做 normalization，使得均值为 0，方差为 1

现在是按每一个样本（按行）来做 normalization，在 transformer 出来之后变得流行

## 步骤替换得到新的 normalization

1. 替换 reshape 成
   1. InstanceNorm: $n \times c \times w \times h \rightarrow wh \times cn$
   2. GroupNorm: $n \times c \times w \times h \rightarrow swh \times gn, c = sg$
   3. CrossNorm: 一对 feature swap mean/ std
2. 替换 normalization 成 whitening
3. 替换 recovery 成一个 dense layer
4. 用在 weights 或者 gradient 上面

# Transfer Learning 迁移学习

因为每次重新训练一个模型的成本很高，可以有以下一些途径复用已有的模型

- Feature Extraction: Word2Vec, ResNet-50 feature, I3D feature
- 直接 reuse 一个 related task's model
- fine-tune 一个 pertrained model

相关的

- semi-supervised learning
- zero shot (无样本)/ few shot learning (少量样本)
- multi-task learning 同时训练多个任务

## fine tuning in CV

利用已经在很大规模的数据集（10倍～100倍）上训练好的模型（pre-trained model），我们认为这个模型是有一定的泛化能力的 (generalize well)，可以做其他任务/在其他数据集上。很多时候比直接在自己的数据集上训练的效果要好，通常不会让精度变低，一般会让 converge 变快

去哪里找？Tensorflow Hub, TIMM

神经网络一般可以分为两个部分：
1. encoder (feature extractor) 将输入 mapping 到 linearly separable features
2. decoder (linear classifier) 做决定，把特征投影到语义空间上。如何划分哪些层是 decoder 是很主观的

- initialize new model: 一开始的 weights 不是随机的，而是把 encoder 部分的 pre-trained model 的所有 weights 复制过来，只有decoder 部分的 weights 是随机的。
  - 只用很小的 learning rate 和很少的 epochs：不要走太远了（regularize search space 来避免泛化能力丢失，以及模型记住了整个小数据集 overfitting 了）
- Freeze Bottom Layers：底层是 low-level features，所以比较 universal，微调的时候可以不动了，层数越大学习率越大

## fine tuning in NLP

NLP 没有一个大规模的标好的数据集，有大量没有标注的文件 (wikipedia, ebooks, crawled webpages ...)

可以生成 pseudo label 然后用作 supervised learning

常见的 NLP tasks
- Language Model 猜下一个词
- Masked Language Model 随机 mask 一个词，根据上下文来猜这个词（完形填空）

常见模型
- word embedding 对每一个词学习两个不同的嵌入 $u_w, v_w$。比如 CBOW 就是使用 $argmax_yu_y^T \sum_i v_{x_i}$ 来预测完形填空（y是要猜的那个词
- Transformer类型
  - BERT encoder
  - GPT decoder
  - T5 encoder-decoder

Bert
- 有两个 pre-training tasks: masked token prediction, next sentence prediction（一次进去两个句子，猜是否在原文相连接）
- 变种：ALBERT, ELECTRA, RoBERTa
- fine-tuning
  - sentence classification: Bert 输出的 CLS embedding 接上一个 dense layer
  - named-entity recognition：是不是人名/街道 hidden layer output
  - Question answering：第一句话是问题，第二句话是reference
  - 非常不稳定，因为 Bert 去掉了 Adam 里面的 bias correction step，在很小的 epoch (3次) 和 dataset 上会非常 unstable
  - 可以在 top layers 的 weights randomly initialize