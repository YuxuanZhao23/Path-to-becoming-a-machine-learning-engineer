# Batch Normalization

- 使得 unit gaussian activation：对每一维计算empirical mean & variance
- 改变的是输入到下一层的数据，不会对weight有任何改变
- vanilla differentiable function: $\hat{x}^{(k)} = \frac{x^{(k) - E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}$
- 一般发生在全连接层后面
- Recovery：如果我们想要保留方差的话，可以通过 $y_j' = \gamma_j\hat{x}_j + \beta_j$ 还原回去，$\gamma_j, \beta_j$ 是可以学的参数。这一步的重点在于做了 batch normalization 是不会有 saturate 的，但是其实有一点 saturate 是好的，所以我们可以控制这一点

# babysitting

- sanity check: crank up regularization -> loss should go up
- 用极少量的数据训练，应该能够很好地 overfitting 接近 100%
- 用很小的 regularization，测试 learning rate，如果 loss 几乎没有变，那么 learning rate 就很可能太低了。loss explode（NaN）就意味着 learning rate 太高了
- cross validation：如果我们发现 loss 大于原本开始的值的三倍的时候，我们可以及早停下。这个hyper parameters的组合很可能是有问题的
- 先期少数几个 epoch 的训练里最好的结果不一定真的好，也要考虑到底这种 hyper parameters 的组合有没有真的 explore 整个 search space
- Random Search 一般会比 grid search 好一点
- 太高的 learning rate 会增加 loss，或者急速下降然后 plateau，其实都是不好的
- 如果 loss 在 plateau 一段时间之后才开始下降，这一般是因为我们的 initialization is bad
- 如果 training accuracy 要远大于 validation accuracy，那么可能是 overfitting 了，需要增加一点 regularization strength。如果没有gap，那么考虑将model变得复杂一点
- 我们可以 track update/weight 的比例，最好是在 0.001 左右，0.01 是还ok，但是太高或者太低都不好