# visualize

- 我们可以 visualize 第一层的 weights，但是一般来说第二层就已经看不懂了。也可以看最后一层 FC 的输入，如果我们使用相同的 nearest neighbor 来比较这些输入，我们会发现同一个 class 的会在向量空间也很接近（尽管 pixel-wise 是相差很远的）
- Principle Component Analysis/ t-SNE 可以把 4096 维降成 2 维
- visualize activation: feature map
- maximally activating patches: 某一层的某个channel，找到最能 activate 的图片（这个channel会有某种追求的目标）
- occlusion experiment：在图片上挖空一部分，看看score怎么变化，变化很大的部分说明应该很重要
- saliency maps：可以用作 semantic segmentation（GrabCut）效果比起有 supervised 会差一点
- intermediate features via guided backprop：如果只是 backprop positive gradient 的话 image 会更好一些
- Gradient Ascent to Visualizing CNN features: $I^* = argmax_I S_c(I) + \lambda || I ||_2^2$
  - better regularizer: L2 norm 是不够的，加上 Gaussian blur, clip small pixel and gradient to 0
  - 可以考虑 multi modality：比如说grocery store 可以是货架的图片，结账的图片，外观的图片，把 mode 抽出来会有更好的效果
- Fooling Images/ Adversarial example：随机选择一张图片，修改它直到骗到模型（一般来说图片看起来对人来说没区别）
- DeepDream：Amplify existing features 图片上会出现很多原本不存在的东西
- Feature Inversion：minimize feature vector 和我们基于 feature 和新图片生成的图片，网络越深，我们越来越不能重建图片（保留了 semantic 部分，pixel low level 都被 throw away 了
- texture synthesis：更大的图片，同样的 texture。传统用 NN，现在有 Gram Matrix 这些 Neural Texture Synthesis
  - Style transfer: content + style => 降低 feature reconstruction loss of content 和 gram matrix loss of style。
    - 可以调整两者 weight 来使得生成的图片更多地保留 style 还是 content
    - style 和 content 的分辨率不一样是没关系的，而且我们可以通过这一点来控制最终的成果
    - 速度很慢，可以训练 feed forward  network 来改善（fast style transfer）

# Unsupervised learning

没有 label 的数据，尝试学数据的 hidden structure

- clustering (K-Means)
- dimensionality reduction
- feature learning
- density estimation

# Generative Models

给定 training data，生成 same distribution 的 new samples

可以 explicitly define class 也可以只是 sample

为什么我们需要 generative model？
- 我们可以制造 time-series data 用于 simulation 和 planning
- 给定草图，可以生成 inference of latent representation

# Pixel RNN/ CNN

- likelihood of image x 
- probability of ith pixel value given all previous pixels
- how to define "previous pixel"? 左边和上边的像素
- maximize likelihood of training data
- 使用 LSTM 来 model，sequential generation 很慢
- CNN 版本是使用 softmax loss at pixel level，训练可以并行，会比 RNN 更快，但是生成的时候仍然需要顺序，所以很慢

$p(x) = \displaystyle\prod_{i=1}^n p(x_i|x_1, ..., x_{i-1})$

# Variational autoencoders

$p_\theta(x) = \int p_\theta(z)p_\theta(x|z)dz$

- 不能直接 optimize，可以 derive and optimize lower bound of likelihood：我们定义一个 $q_\phi(z|x)$ 来近似 $p_\theta(x|z)$
- 我们想要学一个 dimensionality reduction 的 z 来代表 features
- 如果 decoder 能使用 z reconstruct input data，那么我们就认为 encoder 把有意义的内容记录了下来
- 最后使用 L2 loss function 来衡量，所以不需要 label
- 有时候我们可以最后把 decoder 丢弃，然后用很少量的带 label 的数据训练最后一层 classifier 来使用 loss function
- 不同的 z dimension 可以 encode interpretable factors of variation：比如说头朝向的方向，笑容大小
- 可以 inference q(z|x)，也就是生成数据。定义了一个 intractable density 来优化下限（不如 PixelRNN/ CNN 优化的效果好）blur low quality sample（不如 GAN）

# Generative Adversarial Nets

- 如果我们只想要 sample，不想要 explicitly modeling density 呢
- Generator: random noise => sample from training distribution
- Discriminator: distinguish real/ fake
- Minimax game: $min_{\theta_g} max_{\theta_d} [E_{x \in p_{data}}log D_{\theta_d}(x) + E_{z \in p(z)}log(1-D_{\theta_d}(G_{\theta_g}(z)))]$ generator 想要最小化，discriminator 想要最大化
- sample 效果很差的时候，slope 同时很 flat，所以 generator 进步不了
- 所以 generator 按照最大化 discriminator 是错的这个方向来优化：$max_{\theta_g}E_{z \in p(z)}log(D_{\theta_d}(G_{\theta_g(z)}))$，这样就可以学习了，这个是被使用在实践中
- 同时训练两个模型是很有挑战和不稳定的，更改目标函数是很重要的
- 先训练一下 discriminator，随机喂一点 random 数据进去当作 fake
- 可以用 z 做 interpretable vector math，比如说 笑女 - 中立女 + 中立男 = 笑男
- 苹果变成橙子，马变成斑马，手稿变成现实稿，变换季节