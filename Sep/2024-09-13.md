# 为什么我们想要normalize？

我们可以想象用一条直线分两个类，如果这些 data point 都在离原点很远的地方，那么 weight 一点点很小的波动都会导致准确率大幅下降（像杠杆一样）所以我们想要 normalize，让这些点围绕在原点附近。loss function 不会那么敏感

# SGD + Momentum

$v_{t+1} = \rho v_t + \bigtriangledown f(x_t)$

$x_{t+1} = x_t - \alpha v_{t+1}$

```python
vx = 0
while True:
    dx = compute_gradient(x)
    vx = rho * vx + dx
    x += learning_rate * vx
```
- 一般我们会设置 $\rho$ 为 0.9 或者 0.99
- 用于解决 Local minima, saddle points (gradient = 0 的位置不动了), poor conditioning (zig-zag)
- 使用 SGD 的时候因为我们使用了 mini batch，所以 gradient 是 noisy 的
- velocity 是 weighted gradient sum
- SGD + Momentum 有可能会 overshoot 一些 narrow minima，但这很可能是一件好事，我们希望 land on a flat minima，因为更改 training data 很可能就会使得这些 narrow minima 消失，flat minima 很可能更 generalized

# nesterov momentum

- momentum update 是当前位置的 velocity 和 gradient，可以改用 nesterov momentum，先走了 velocity 这一步，使用终点的 gradient
- 对于 nesterov momentum，我们的 update 会基于 $\bigtriangledown f(x_t + \rho v_t)$ 但是我们希望它是基于 $\bigtriangledown f(x_t)$，所以我们换了变量名来处理：$\tilde{x}_t = x_t + \rho v_t$，得到：

```python
dx = compute_gradient(x)
old_v = v
v = rho * v - learning_rate * dx
x += -rho * old_v + (1+rho) * v
```

# AdaGrad

```python
grad_square = 0
while True:
    dx = compute_gradient(x)
    grad_square += dx * dx
    x -= learning_rate * dx / (np.sqrt(grad_sqaure) + 1e-7)
```

- 减少在gradient很快的方向上的行进速度，增快很慢的 gradient 方向
- 随着时间一直累加 gradient square，所以走的步子越来越小（因为分母越来越大），在 convex 的情况下比较好，可以比较好地 converge
- 实际中使用的比较少，就因为它很容易停下来这点
- 这个 ```1e-7``` 是啥意思？我们需要分母不为 0，所以我们初始的时候放一个很小的正数进去

# RMSProp

类似于 AdaGrad，只改 ```grad_square``` 的更新方式

```python
grad_square = decay_rate * grad_square + (1- decay_rate) * dx * dx
```

也有点类似于 momentum，换成了基于 gradient sqaure 而已，所以我们就不会必然得到一个越来越小的 step，避免了非 convex 就会中途停下的问题

# Adam

既使用 momentum 和 AdaGrad/RMSProp，会不会更好呢？

```python
first, second = 0, 0
while True:
    dx = compute_gradient(x)
    first = beta1 * first + (1 - beta1) * dx # momentum
    second = beta2 * second + (1 - beta2) * dx * dx # RMSProp
    first_unbias = first / (1 - beta1 ** t)
    second_unbias = second / (1 - beta2 ** t) # bias correction
    x -= learning_rate * first_unbias / (np,sqrt(second_unbias) + 1e-7)
```

- 如果我们把 second initialize 成 0，我们就会在一开始无论具体情况如何都会 step 很大，这样很不好
- 所以增加 bias correction 这一步来消除前面大踏步走到了非常差的地方的风险
- 任何模型都可以用这个设定开始试试：beta1 = 0.9，beta2 = 0.999, learning_rate = 1e-3 or 5e-4
- 什么问题是连 Adam 都无法解决的？我们还是在按照 coordinate 的方向一个一个去优化，如果是一个rotate的输入，那么我们很难通过单一方向的和得到一个很好的结果，上述的方法都不能

# learning rate

- 我们可以不同的阶段使用不同的 learning rate (decay)
- 主要用在 SGD momentum
- 我们需要先不使用 decay，然后观察哪个位置可能使用 decay 会有帮助，我们已经到了 plateau，降低 learning rate 可能可以进一步有 progress

# Second order optimization (review)

- 如果可以接受 full batch update，那么 L-BFGS 是可以使用的，但是需要 disable all sources of noise

# bagging

- 使用不同的 seeding 开始，然后 average 结果基本上会拿到 2% 以上的进步
- 也可以只是保留训练阶段中的多个 snapshot，不用多次训练啦