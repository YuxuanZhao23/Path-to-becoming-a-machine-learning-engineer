# Linear Regression

使用 training set 来训练一个 algorithm $h(x) = \sum_{j=0}^n\theta_j x_j$，其中 $x_0 = 1$

- $\theta$ parameters
- m training samples
- x inputs/ features
- y output/ target variable
- $(x^i, y^i)$ i-th training example
- n features

如何选择 $\theta$ ？找到能够使得 $h(x) \approx y$

数学表达是我们要 minimize $J(\theta) = \frac{1}{2} \sum_{i=1}^m(h(x^i) - y^i)^2$

# Gradient Descent

随便选定一个 $\theta$，朝着减小 $J(\theta)$ 最快的方向改变 $\theta$

$\theta_j := \theta_j - \alpha \frac{d J(\theta)}{d \theta_j}$

通过带入上面的表达式，我们可以展开的得到 $\frac{d J(\theta)}{d \theta_i} = (h_\theta(x^i) - y^i) * x^i$

# Batch Gradient Descent

对于很大的数据集，我们不想要看一遍所有的数据才能开始更新$\theta$ (Batch)，所以我们使用 stochastic gradient descent，也就是只计算某一个小部分的结果就更新 $\theta$