# LeNet-5

- 第一个成功的 CNN
- conv filter 5 x 5, stride 1
- pooling 2 x 2, stride 2
- (CONV - POOL) * 2 - FC * 2

# AlexNet

- 第一个比所有非 deep learning model 都要强
- (CONV - POOL - NORM) * 2 - CONV * 3 - POOL - FC * 3
- CONV1 [227 x 227 x 3] images, 96 filters 11 x 11, stride 4 => parameters 11 * 11 * 3 * 96, Output [55 x 55 x 96]
- POOL1 3 x 3 filters, stride 2 => parameters 0, Output [27 x 27 x 96]
- 第一次使用 ReLU
- Norm 现在不再被使用
- heavy data augmentation
- dropout 0.5
- batch size 128
- SGD Momentum 0.9
- learning rate 1e-2
- L2 weight decay 5e-4
- 7 CNN ensemble: 18.2% => 15.4%
- 模型的图片有两行，主要是因为当时的 GTX 580 只有 3GB memory，所以只能把整个网络一分为二，每个GPU上有一半，CONV3 和 3个 FC 才共通，其他时候都是参数不互通

# VGGNet

- 16 ~ 19 layers
- 3 x 3 conv, stride 1, pad 1
- 2 x 2 max pool, stride 2
- 为什么使用更小的 filter？三个 3x3 effective receptive field 等同于一个 7x7，但是同时可以有更多的 non-linearities 和更少的 parameters
- 为什么更深的网络里面往往 size 比较小，因为这是CNN自然而然的，同时减少一点parameters的增加
- 没有 local response normalization
- 使用了 ensemble
- FC7 很适合用于其他的 tasks 的 generalization
- 最高的 memory 和 operations，最不高效

# GoogleNet

- 22 layers
- 非常高效的 inception module（参数很少）
- no FC layer
- naive inception module：同一层有不同 size 的 conv 和 pooling，把结果 concatenate 起来，这样参数会太多
  - 因为pooling layer会保持feature depth，所以参数只会越变越多
  - Bottleneck (GoogleNet 改进)：使用 1x1 conv 来提前降低 channel (depth)
- 中途有两个 branch 分支出来做 classification，这是为了给这个很深的网络中的比较前面的 layer 更多的训练 loss

# ResNet

- 152 layers
- 直接叠加更多层在普通 cnn 上有用吗？training error/ testing error 结果会更差（不是 overfit）很难拟合
- 但是理论上应该更深的网络表现应当大于等于更浅的网络，因为可以把一些层设成 identity mapping
- residual connection：在 ReLU 之前我们会把 x 加到结果（让我们学一下我们需要给输入加或者减多少），网络其实 fit 的是 $\Delta x = H(x) - x$
- 所有的 residual block 都是 3 x 3 conv layers
- 时不时进行 filter 加倍，stirde 2 进行 downsample
- 开头有额外的 7 x 7 conv layer
- 同样使用了 1 x 1 conv 来做 bottleneck，用两个 1 夹住一个 3 来保持 depth
- 没有 FC
- 每个 conv 之后都有 batch normalization
- Xavier/2 initialization
- SGD + Momentum 0.9
- learning rate 0.1 => plateau 就再除十
- mini batch 256
- weight decay 1e-5
- 没用 dropout
- 超越 human performance 3.6%
- 改进：
  - x 可以跳更远
  - 可以有更多的 filters 在同一层（wider），而不是更深的网络，可以 parallel
  - res-next：多个相同结构的 path
  - 避免 vanishing gradient：随机 drop 一些 layer