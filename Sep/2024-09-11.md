# Sigmoid

- 可以 squash numbers to [0, 1]，曾经流行，因为它和神经元 firing rate 很像
- 图像：x = 0 的时候在 1/2，然后正方向上极速升高到1，负方向降低到0

缺点：
- 如果 x = -10, gradient = 0，这个时候根据chain rule，就会所有经过的斜率都变成0
- 如果 x = 10, gradient = 0 同样的情况就会发生
- 输出不是 0-centered（背景：如果对于一个 neuron 所有的输入都大于零，那么 gradient 也会要么所有都是正数/负数，也就是说梯度下降的方向是zig zag的（没办法真正走最快的学习方向），这也是为什么我们想要 0-mean data）
- exp() 是一个很贵的运算

# tanh

- 主要的区别是 0-centered，范围是 [-1,1]
- 仍然会kill gradient flow when saturated

# ReLU

- 不会 saturated
- 非常容易计算
- 6x converge speed
- more biologically plauible than sigmoid
- AlexNet 用的是这个
- 一般大家会在 initialize weights 的时候设置成非常小的正数

缺点

- not 0-centered output
- x < 0 的时候同样有 saturate 的问题

ReLU 训练不了有两种情况

- weight seeding 初始值使得 activate layer dead = 0
- seeding 的位置使得训练的方向错了，越走越歪到第一种情况
- learning rate 太高所以弹跳到上面的两种情况中

# Leaky ReLU

- 不会 saturate
- 计算仍然很简单 $f(x) = max(0.01x, x)$
- 6x converge speed
- will not die like ReLU
- 变种 Parametric Rectifier (PReLU) $f(x) = max(\alpha x, x)$ 其中 $\alpha$ 是可以学的参数

# Exponential Linear Units (ELU)

- $f(x) = x, x > 0$
- $f(x) = \alpha (exp(x) - 1), x \leq 0$
- 更加接近 0-mean
- 所有的 ReLU
- Robust to noise
- 计算量会大一点

# maxout neuron

- $max(w^T_1x + b_1, w^T_2x + b_2)$
- 缺点：double parameter

# Data preprocessing

- zero-centered
- normalized 图片一般不需要
- 一般不做 decorrelated, whitened