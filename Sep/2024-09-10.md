# Gradient

什么是 gradient？对于 scalar 来说，derivative 是 slope。而 gradient 就是对于 vector 来说的 slope

当我们想要往下走的时候，- gradient 是最快 descent 的方法

我们不会使用 numerical gradient 去逼极限，因为这样会花费很多时间去计算。现实中都是使用 analytic gradient 的方法，这样能得到准确快速的结果。我们可以手动算一些 numerical gradient 来验证结果是对的

# Gradient Descent

```python
while True
    weights_grad = evaluate_graident(loss_func, data, weights)
    weights += - learning_rate * weights_grad
```

# Stochastic Gradient Descent (SGD)

计算所有的样本的 loss 实在是太贵了，所以我们可以使用 minibatch 来学习（常用：32/64/128）

```python
while True
    data_batch = sample_training_data(data, 256)
    weights_grad = evaluate_graident(loss_func, data_batch, weights)
    weights += - learning_rate * weights_grad
```

# Image Features

我们直接给 linear classification 喂 raw data 的效果可能很差，所以我们可以将数据转化成一些不同的 features，
- polar coordinate 来重新分布我们的数据
- Histogram of Oriented Gradients (HoG)
- Bag of words (code book of visual words)

# Back Propagation

chain rule

$\frac{dy}{dx} = \frac{dy}{dz}\frac{dz}{dx}$

$\frac{dy}{dx} = \sum_i\frac{dy}{dz_i}\frac{dz_i}{dx}$ 如果 x 和一系列的 z 都有关系的话，结果是全部加起来

Sigmoid function

$\sigma(x) = \frac{1}{1 + e ^ {-x}}$

$\frac{d\sigma (x)}{dx} = (1 - \sigma(x))\sigma(x)$

- add gate: gradient distributor 不改变
- max gate: gradient router 只选一条branch
- mul gate: gradient switcher 换过来

现实中我们不会给每一个运算过程维护一个 Jacobian matrix，因为这样会占用太多的空间，变得很低效。

- forward: 计算并保存所有 intermediate 的结果，方便之后计算 gradient 的时候可以直接在 memory 里调用
- backward: 使用 chain rule 来计算 gradient of loss function

# Biological Neuron

- 现实中的神经元的impulse传递有点类似于 ReLU
- neuron 有很多种类
- dendrite 可以进行复杂的非线性计算
- synapse 不是一个 weight，而是一个复杂的动态的非线性系统
- rate code 很可能是不充分的

# Activation Function

- Sigmoid
- tanh
- ReLU
- Leaky ReLU
- Maxout
- ELU

# Convolutional Neural Network CNN

对于图片来说，我们的 filter 将会有这相同的 depth：图片是 $32 \times 32 \times 3$，那么filter 可以是 $5 \times 5 \times 3$，最后这个RGB 3 channel是不会变的，CNN是为了slide over 所有的空间信息。最后我们会得到一个 $28 \times 28 \times 1$，但是我们可以有多个filter，如果我们有6个不同的filter，这样我们就会有 $28 \times 28 \times 6$

convolve的含义：实际操作中，我们会把这一块抠下来，展平之后做dot product，其实就是对应位置上的x和w相乘之后相加在一起得到一个值。然后我们会 slide 和这个filter

我们可以选择 slide 的 stride (step) 上面的例子里 stride = 1，但是其实可以取别的值。Output size (N-F)/stride + 1。stride时大时小可以吗？本质上没有问题，但是一般不会这么做

需要的时候我们可以用 0 pad border（0不是必须的，可以mirror旁边的数字，但是0的表现是ok的）padding 的好处还有：不会因为网络比较深就很快把大小shrink得太小

有多少个可学习的参数？ (filter size $\times$ filter size $\times$ depth + 1 (bias)) * filter 个数

# Pooling layer

((Convolve $\rightarrow$ Activate) * M $\rightarrow$ Pool?) * N $\rightarrow$ (Fully Connected Layer $\rightarrow$ RELU) * k $\rightarrow$ Softmax

n <= 5, m is big, 0 <= k <= 2

使用某种方式来 downsample，比如说 max pooling，pooling之后的图片分辨率变小，但是depth保持不变

一般来说不会有overlap，所以stride 和 size 会相同，同时 pooling 一般不会使用 0 来做 padding