# 提升单一模型性能

实践：往往使用 batch normalization，不行再加上 dropout

## Regularization

在 loss 里面加入一项 $\lambda R(W)$，可以是 L2/L1/Elastic net

## Data Augmentation

translate/ rotate/ stretch/ shear/ lens distortion
- image horizontal flip
- crop
- scale
- color fitter: randomize contrast and brightness
- PCA on RGB
- color offset
- offset to all pixels

## Dropout

在每一次 forward 中随机将一些 neuron 设成 0，我们可以设置这个 dropping 的比例，一般是 p = 0.5

```python
U1 = np.random.rand(*H1.shape) < p
```

- 可以理解成我们每次都在训练一个不同的 subset model
- 是在 activation 之后，不影响weight
- 一般发生在 Fully Connected Layer/ Convolutional layer（drop 掉一整个 channel）
- 原理：强迫模型有 redundant representation，避免 co-adaptation of features
- 也可以理解成每一个binary mask是一个单一模型，所以我们 ensemble 了一个巨大的 model
- 如何测试呢？如何 average out randomness？我们可以在 test time 乘上这个 dropout probability，这样期望值就会相等。也可以在 train 的时候除以 p ```U1 = (np.random.rand(*H1.shape) < p)/p```，加快test time
- 训练会变慢，一位每一次都只训练一部分
- 效果上其实类似于 batch normalization，都同样需要在 test 的时候 average out 这种 randomness，但是 dropout 的好处在于有一个可以调节的 p

## DropConnect

zero out some of the weight matrix (改变fully connected network)

## Fractional Max Pooling（不常见）

Pooling 大小不一

## Stochastic Depth（不常见）

有时候会 drop 掉一些 layer

## Transfer Learning

基本上 CNN 都是利用 pretrained model

|  | dataset 很类似 | dataset 很不一样 |
| -- | -- | -- |
| 数据很少 | top layer 改成一个新的 linear classifier | 在不同阶段使用 linear classifier |
| 数据很多 | fine tune 一些层 | fine tune 很多层 |