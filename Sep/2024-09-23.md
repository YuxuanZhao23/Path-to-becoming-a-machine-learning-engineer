# Bidirectional Encoder Representations from Transformers BERT

第一个在大量数据上训练的语言模型，可以 fine tune 做多种任务

- bidirectional 体现在哪里？和 GPT 相比，BERT 不仅仅会看左边的内容，也会看右边的内容
- ELMo 使用的是 RNN，BERT 用的是 Transformer
- pre-training 可以有两种 level：sentence 和 token level
- pretraining 可以有两种 strategies：feature-based（针对任务设计一个模型，使用与训练作为额外的feature） 和 fine-tuning（只加很少的参数/一层）
- 使用 Masked Language Model MLM 来做完形填空（token level），还有预测两个句子是否相邻（sentence level）正负例各 50%
- 在很多的无标号数据上训练的结果很可能会比在少量有标号数据上训练要好，所以 BERT 的 pretraining 是在无标号的数据上，之后直接使用这些 parameters 来 fine tuning 在有标号的数据上
- 可学习参数主要有两部分：
- embedding（字典大小 30k * 隐藏单元 Hidden Unit H） 
- transformer，以下两个部分需要乘上 L
  - 自注意力机制：本身是没有可学习参数的，但是多头注意力会将 Key，Value，Query 做一次投影，输出也需要一次投影，头的个数 $A * 64 = H$，所以这里是 $4 H^2$
  - MLP，两个 FC，一个输入 H 输出 4H，另一个输入 4H 输出 H，加在一起就是 $8H^2$
- 可以输入一个 sentence 或者一对 sentences，sentence 在这里只是 arbitrary span of contiguous text，不一定是 linguistic sentence。如何输入一对呢？```[CLS] Token1 ... TokenN [SEP] Token1 ... TokenM```
- 在做字典的时候，并不会用空格来分，因为这样字典很容易就变得太大，使用的是 WordPiece，如果一个词出现的频率不高，那么就看它的部分是否是词根，以此来减少字典大小和可学习参数的规模
- BERT 里面不是手动构造的哪个部分属于第一/二个句子，以及每个 token 的位置信息，全部都是学习得来的
- 怎么做的 Mask？对于在 WordPiece 里面的 token，会有 15% 的概率随机 mask。对于这 15% 的词，80% 会被替换成 [MASK]，10% 会被替换成另一个随机的 token（噪音），10% 保持原样（因为微调的时候是没有 [MASK]）
- BERT 利用率是 GPT 的十倍

# Transformer

- 连接多头注意和 MLP 的是残差连接
- 使用 LayerNorm 而不是 Batch Norm，原因是每一个 sequence 的长度时不相等的所以只对当前的这个 sequence 和它的 feature 做 normalize 会比较好。因为是对每一个样本做的，所以不需要存全局的方差，也不担心长度不一的问题
- Regularization: Residual Dropout, label smoothing (soft Max 需要无穷大才能逼近 1，所以可以把目标降低一点， transformer 降到了 0.1)

Encoder: Input embedding + Positional encoding => [Multi-head Attention + Add & Norm => MLP + Add & Norm] * 6

- 什么叫做自注意力机制？就是说同样的东西既作为 query，也作为 key 和 value
- 为什么要使用自注意力机制？

|层类型|计算复杂度|顺序操作|最远距离|
| --- | ------ | ----- | ----- |
| Self Attention | O($n^2 d$) | O(1) | O(1) |
| RNN | O($n d^2$) | O(n) | O(n) |
| CNN | O(k n d^2) | O(1) | O($log_kn$) |


Decoder: Output embedding + Positional encoding => [Masked Multi-head Attention + Add & Norm + Encoder Result => Multi-head Attention + Add & Norm => MLP + Add & Norm] * 6 => Linear + Softmax

- 为什么 decoder 需要先通过一个 masked 多头注意力机制？因为不能提前看到后面的内容
- decoder 里面的第二个 attention，key 和 value 来自于 encoder 的输出，query 来自于 decoder 的 masked multi-head attention 的输出。也就是说根据 decoder 的输入我们在 encoder 的输出中挑感兴趣的内容，不感兴趣的东西可以忽略

# Attention

对于每一个 query，使用 compatibility function 来计算它和 keys 的权重，最后得到的 output 是这个权重和 values 的加权和

Transformer 使用的是 Scaled Dot-Product Attention

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

其实就是计算 cos 的大小，为什么需要除以 $\sqrt{d_k}$？因为 dk = 512 比较大，所以可能会让一部分太趋近于1，另一部分太趋近于0，所以学不动。

Mask: 对于 $q_t$ 来说，我们只应该看到 $k_1, k_2, ..., k_{t-1}$

会发现正常的 attention 是没有可以学的参数的，但是 transformer 使用的多头注意力是先用高维投影到低维，然后再做 scaled dot-product attention，这样的投影有 8 组，concat 结果再通过 linear 层输出