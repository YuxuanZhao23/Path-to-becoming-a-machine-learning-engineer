# Markov Decision Process

Markov property: $(S, A, R, P, \gamma)$

- S: possible states
- A: possible actions
- R: distribution of (state, action) => reward
- transition probability state => next state
- $\gamma$ discount factor
- 目标找到一个 policy $\pi^*$ 能够最大化 cumulative discounted reward $\sum_{t > 0} \gamma ^ t r_t$
- Bellman equation：最大化每一步的 reward，问题就是这样需要计算每一个 Q(s, a)，性能会很差
- 使用神经网络来 estimate $Q(s, a, \theta)$

# Experience replay
- 持续更新（玩一个游戏多次 episode）一个 replay memory table 来记录 $(s_t, a_t, r_t, s_{t+1})$，更新的时候小概率选择一个随机 action，大概率使用当前 policy greedy action
- 训练 q 网络的时候使用 replay memory 的 random minibatch 而不是 consecutive samples，这样可以避免 inefficient learning 和 bad feedback loop

# Reinforcement learning

- agent interact with environment, maximize reward
- Cart-Pole problem: angle, angular speed, position, horizontal velocity, reward is upright
- Robot locomotion: man/ ant
- Atari Games
- Go 围棋

# Policy Gradient

- Reinforce algorithm $J(\theta) = \int_\tau r(\tau)p(\tau; \theta)d\tau$ 我们可以使用 Monte Carlo sampling 来 estimate $\bigtriangledown_\theta J(\theta) = E_{r \in p(\tau; \theta)}[r(\tau) \bigtriangledown_\theta log p(\tau; \theta)]$，使用这种方法我们不需要知道 transition probabilities。直觉就是如果我们得到比较好的结果 $r(\tau)$，那么已经采取的行动都被认为是好的，反之则反
- 然而最终这种奖励和惩罚都互相抵消了，结果会有 high variance 因为 credit assignment 是很难的
- 需要很多 samples，因为 sample efficiency 不好
- 只可以 converge 到一个 local minima $J(\theta)$ 很多时候已经 good enough

# Q-Learning

- 降低 variance：设置一个 baseline estimation，所以现在衡量的是有没有惊喜和意外，而不是单纯的好坏。
- 可以使用当前所有的 constant moving average rewards
- 也可以使用 Q function 和 value function：$Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)$
- 我们可以使用 Actor-Critic Algorithm 在不知道 Q 和 V 的前提下学习：actor 是 policy，critic 是 Q-function，我们使用 advantage function 来定义一个 action 比期望中要好多少：$A^\pi (s, a) = Q^\pi (s, a) - V^\tau (s)$
- 如果可以 converge 的时候一般更加 sample-efficient，有exploration 的问题
- 不一定能有结果，因为使用了一个很复杂的 function approximate Bellman equation

# Recurrent Attention Model (hard attention)

- 只看一眼图片，决定下一步看哪里，可以潜在省很多资源和忽略杂乱的不相关内容，不用看一整张图片
- 使用 RNN，传递下一个要看的坐标，softmax 预计结果正确的话 reward = 1

# AlphaGo

- supervised learning + reinforcement learning
- self-playing
- policy and value network => Monte Carlo Tree Search Algorithm

# Algorithm Inference

- Pruning + retraining：去掉不需要的 weight 连接，1/10的计算量可以有相似的效果
- weight sharing (discrete weight)：cluster weights 生成一个 code book，量化 code book 的 weights，retrain code book，可以使用 1/8 的空间（就像使用16bit的浮点数代替32bit，太高精度其实没什么用，只会 overfit 和浪费大量读写时间）
- Huffman coding：常用的 weight 使用更少的 bit 来表示
- Quantization：在知道了使用的数字的最大值最小值之后合理分配 bit 来表示整数和小数部分，压缩32bit到8bit一般没有什么精度影响
- Low Rank Approximation：把 CNN decompose 成更小的多个 filter，用 SVD 把一个 FC 分成两个
- binary/ ternary quantization: 训练正常训练，inference的时候直接变成2/3个weight
- Winograd Convolution

# Hardware Inference

- 使用专用的硬件类似于 TPU 可以比通用处理器有几十上百倍的速度提升/能耗
- 很多时候我们的需求是低延迟的，所以我们不能 batch 很多，导致很低的 ops/byte
- 如何解决？compress model，然后使用能够在 compress model 上 infer 的 model
- sparse weight (0 * A = 0), sparse activation (W * 0 = 0), weight sharing

# Algorithm Training

- data parallel: limited by batch size
- model parallel: cut the model by conv layer, FC
- Mixed Precision with FP16 and FP32 (4x energy/ space)只在 weight update 的时候使用 FP32
- Model distill：使用成熟模型来教新的模型，我们需要 soften output，可以使用 temperature 来决定 soft 多少
- DSD 先剪枝，后补上

# Hardware Training

- computation + bandwidth 都要很好
- Streaming Multiprocessor
- Tensor Core