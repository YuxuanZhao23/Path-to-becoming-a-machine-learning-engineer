# CLIP

- 其实是一种双塔模型
- 因为embedding可以提前做好，所以实际inference的时候只需要做点乘就好，速度就会很快
- 但是只适合做大规模的retrieval这一个任务

# Align Before Fuse ALBEF

经验：
- 更大的 Vision Transformer ViT（应当比文本 embedding 要大） 
- 更大的 Modality Interaction
- 不要 Objection Detection 和与之相对的 Word Patch Alignment：性能太差，而且这样使得文本和图片之间 unaligned，所以也不需要 bounding box annotation 和高分辨率图片
- 类 BERT 的 Mask Language Modeling MLM 是好的
- 类 ViLT 的 Image Text Matching ITM 是好的
- 类 CLIP 的 Image Text Contrast ITC loss 来训练

momentum model 生成 pseudo-target $\rightarrow$ momentum distillation (self-learning) noisy web data. 
- 为什么需要处理 noisy web data？因为使用的是alt text，往往不是准确的图片描述
- pseudo target 是什么意思？mask 的文本可能 ground truth 都没有模型预测的好，所以我们不应该只按照 ground truth 来惩罚模型。momentum model 就能够生成很多合理的填空

模型架构：

image 使用 12 层的 transformer，text 使用 6 层。两者都得到一个 $768 \times 1$ 的 CLS embedding，downsample 到 $256 \times 1$ 之后这两个是正样本。使用 momentum model 提供好几万的负样本，因为这些负样本不做 gradient 所以空间占用问题不大。在这里做 ITC 学习好 18 层。之后训练 multimodal encoder 的 6 层 transformer，ITM 是很容易分辨负样本的，所以我们使用 ITC 的 embedding，计算与正样本最接近的负样本作为 hard negative 给 ITM 学习，这样 ITM 能学得更好。同时我们做另外一次 forward 是 mask 一部分文本的，用 MLM 来补全 mask 的内容。和普通 nlp 对比这种方法也会用到图像的信息来帮助填空

# Vision Language pretrained Model VLMo

当时多模态的数据不够，所以可以用对应的数据单独训练文本和图片的模型 (stagewise pre-training strategy)

Mixture of Modality Expert:

- 使用 Vision Expert, Lnaguage Expert, VL Expert FFN 代替了在传统的 transformer 架构里面的那一个 FFN
- self attention 是同一个 shared weight，不挑输入，最少的 induction bias
- 好处是只有一个模型，根据需要做的下游任务的不同来选择如何使用 expert

模型同样使用了 ITC, ITM, MLM

- ITC 使用了标准的 12 层 CLIP（图片用 V-FFN，文本用 L-FFN）
- ITM 也沿用了 ALBEF 的 LTC 提供 hard negative mining，前十层是 V-FFN 和 L-FFN 并行的，之后融合在一起过两层的 VL-FFN
- MLM 也是和 ITM 一样的 10 + 2

训练流程：

- 先用 BEiT 训练 V-FFN
- 然后冻住了 V-FFN 和 attention（不需要 fine-tuning 就能表现很好，但是如果先训练L-FFN 的话就无效），然后训练 L-FFN
- 最后输入多模态的数据，打开所有的参数接受训练，包括 attention

# Bootstraping Language-Image Pre-training BLIP

BLIP 这个模型借鉴了 ALBEF 做了相同的ITC，ITM结构，又借鉴了 VLMo 的共享注意力机制
- text encoder 的 Bidirectional Self-Attention 和 image-grounded text encoder 上层用的是共享的
- Language Model LM 是一个 decoder 所以有自己的 causal self-attention
- LM 和 image-grounded text encoder 的 cross attention 是共用的
- text encoder, image-grounded text encoder, decoder 的 feed forward 也是共用的
- 这里的 LM decoder 是类似于 GPT 那种预测下一个词而不是之前的 MLM 的完形填空
- 训练需要3次forward

模型：encoder模型不能text generation，encoder-decoder 模型不能 image-text retrieval。训练一个 captioner 和 filter。LM finetune 生成的内容甚至比ground truth的文本还要好。LM 可以生成 synthetic text，再通过ITC和ITM filter选择两个label中更好的一个用来训练

数据：CLIP, ALBEF, SimVLM 都是在大规模的image-text pair嘈杂数据集上训练的。使用添加了pseudo label和filter后生成的好数据训练，模型的提升非常显著。而且可以这个优化是独立的，可以优化任意数据，给不同大小的BLIP或者别的模型使用

例子：Laion 优化数据的标号
1. 用 blip L/14 生成 40 个 caption
2. 用 Clip L/14 排序选前五个
3. 用 ResNet50x64 Clip 选择最好的
4. 用 T0 修复 grammar 和 punctuation

# Contrastive Captioners CoCa

类似于 ALBEF，减少 forward 的次数。区别在于文本这边只使用 decoder，不计算 ITM 和 MLM，而是计算 captioning loss（类似于LM loss）。同时在image encoder 后面接了一个 attentional pooling

# BEiTv3

把图像也当作一种语言，只用 mask modelling 的 loss。不是说 loss 数量越多越好的，也得看这些 loss 之间有没有互补的特性。数据也不是越多越好的，BEiTv3的训练量/数据量要比 coca 少很多，但是效果反而更好。实现用的就是 VLMo，loss 是 Masked Data Modelling