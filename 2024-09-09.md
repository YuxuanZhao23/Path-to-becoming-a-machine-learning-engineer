What is AdaBoost?
Amazon Mechanical Turk: sort, clean, label data

# Nearest Neighbor (NN) & K-NN

如何定义距离：

- L1 Manhattan：在什么情况下使用这个？当我们的坐标系有真实世界的意义的时候，我们可能更加在乎坐标轴方向上的数据。现实中我们可以测试对比 L1 L2 的实际效果来决定使用哪一个
- L2/ Euclidean distance，改变坐标轴不会改变 L2 距离

几乎从来没有在现实中使用：

- 时间复杂度：构建 O(1) 预测 O(N)
- 如果图片 boxed, shifted, tinted 的话，L2距离会不够 informative
- curse of dimensionality：我们需要 training dataset 几乎完全布满所有的 dimension，而这是非常不可能的

# 选择超参数的方式

1. 找出在当前数据集上表现最好的超参数：不好，对于 knn 来说 k=1 就会有最好的效果，没有泛化能力
2. 将数据集分成train和test，使用在test dataset上表现最好的超参数：不好，这样的话在 test dataset 上做出来的结果将没有代表性，因为这样的超参数选择可能会在别的新测试集上表现糟糕
3. 将数据集分成train, test, validate dataset，用train dataset来训练，用validate dataset来找最适合的超参数，这时test dataset是模型从来没有见过的，所以得到的表现结果是有代表性的
4. K-fold Cross validate：实际使用不是特别常见。把数据切分成 k 份，每次选用其中的一份当作 validate dataset，其他的用作 training dataset。重点：test dataset 自始至终都没有改变！

# Linear Classification

这里的线性的意思是描述我们如何构建这个将图片输入变成 class name 的过程: $f(x, W) = Wx + b$

这里的 bias 的作用是什么？如果输入的 dataset 不同的 class 的 input 数量悬殊，比如说猫的图片很多，那么 bias 里对应猫的 index 的数字也会比较大来 compromise 这一点

# Loss Function

我们需要一个方法来指示当前的 weights 表现得有多不好

整个dataset的loss就是每一个样本的loss之和：$L = \frac{1}{N}\sum_i L_i(f(x_i, W), y_i)$

- Multiclass SVM
  - $L_i = \sum_{j \neq y_i}max(0, s_j - s_{y_i} + 1)$
  - 当错误答案比正确值的预测值要小1或以上，loss就为0，我们认定这个1是安全边界
  - 这个1是怎么来的？其实选取任何别的数字都无所谓，因为我们可以对整个weight做normalize，这个时候任何其他数字作为安全边界都一样
  - 假设我们找到了一个有着 0 loss的W，它是unique吗？不是的，比如说2W也会有0 loss